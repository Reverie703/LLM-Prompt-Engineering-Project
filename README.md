# LLM-Prompt-Engineering-Project
This project explores how a language model responds to different prompt formulations. A small decoder-only model is trained on synthetic data, and its behavior is evaluated using paraphrased prompts to study prompt sensitivity and response consistency.
The goal of the project is to gain hands-on experience with prompt engineering, dataset creation, and basic language model training rather than building a large-scale production system.
The trained model is tested using multiple semantically similar prompts, such as different ways of asking for the capital of a country. The responses are compared to observe how small changes in phrasing affect model output.
This helps demonstrate why prompt design matters when working with language models

Author
Adit Rao
